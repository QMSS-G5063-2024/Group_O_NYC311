test1 = lm(Radius_200m ~ Daily_Temperature_.C. + festive_holiday +
med_hh_inc + poverty_rate + unemployment_rate + total_population +
pct_hispanic_or_latino + pct_black + pct_american_indian +
pct_asian + pct_hawaiian_pacific + pct_other + pct_two_or_more_races +
daily_rain_mm + daily_ave_cloud_coverage_pct +
Sub_borough_Area,
data=hyperlocal_data)
test2 = lm(Radius_500m ~ Daily_Temperature_.C. + festive_holiday +
med_hh_inc + poverty_rate + unemployment_rate + total_population +
pct_hispanic_or_latino + pct_black + pct_american_indian +
pct_asian + pct_hawaiian_pacific + pct_other + pct_two_or_more_races +
daily_rain_mm + daily_ave_cloud_coverage_pct +
Sub_borough_Area,
data=hyperlocal_data)
test3 = lm(Radius_1000m ~ Daily_Temperature_.C. + festive_holiday +
med_hh_inc + poverty_rate + unemployment_rate + total_population +
pct_hispanic_or_latino + pct_black + pct_american_indian +
pct_asian + pct_hawaiian_pacific + pct_other + pct_two_or_more_races +
daily_rain_mm + daily_ave_cloud_coverage_pct +
Sub_borough_Area,
data=hyperlocal_data)
stargazer(test1, test2, test3,
keep = c("Daily_Temperature_.C."),
covariate.labels = c("Daily Average Temperature (Celsius)"),
type="text")
hist(hyperlocal_data$NYC311_Calls_200m)
hist(hyperlocal_data$NYC311_Calls_500m)
hist(hyperlocal_data$NYC311_Calls_1000m)
hist(hyperlocal_data$Radius_200m)
hist(hyperlocal_data$Radius_500m)
hist(hyperlocal_data$Radius_1000m)
stargazer(t1.p1.r4, t1.p2.r4, t1.p3.r4,
keep = c("Daily_Temperature_.C."),
covariate.labels = c("Daily Average Temperature (Celsius)"),
type="text")
stargazer(test1, test2, test3,
keep = c("Daily_Temperature_.C."),
covariate.labels = c("Daily Average Temperature (Celsius)"),
type="text")
stargazer(test1, test2, test3,
keep = c("Daily_Temperature_.C."),
covariate.labels = c("Daily Average Temperature (Celsius)"),
type="text")
summary(test3)
#################### ALL REGRESSION RESULTS CODE: NOTES ####################
# since my outcome variables all have a high proportion of zeros,
# and since my outcome variables are all count variables that are over-dispersed (variance higher than mean),
# I will be using zero-inflated negative binomial regressions.
# I will be running several regressions:
# 1. Hyperlocal Monitor Radius specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv"
# 2. 1-1 specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv"
# 3. NTA specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nta_ALL_CONTROLS.csv"
# 4. AQS Monitor radius specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Data/AQS related/AQS_nyc311_ALL_CONTROLS.csv"
# Each of these regressions take up a single table,
# each table has 2/3 panels for each distance range (200m, 500m, 1000m)
# Tables for 1. and 2. have the following 4 regressions and columns:
### baseline controls: just control for federal holidays
### baseline: add socioeconomic & race controls
### baseline: add weather controls
### baseline: add SBA fixed effects
# Table for 3. and 4. has the following 3 regressions and columns:
### baseline controls: just control for federal holidays
### baseline: add socioeconomic & race controls
### baseline: add weather controls
# can't add SBA fixed effects as the independent variable (temperature) would vary at the
# same level as the SBA fixed effect.
# for AQS regressions, there are only 2 monitors and 2 SBAs, and dependent variable
# would vary at the same level as SBA fixed effects, induce multicollinearity
#################### LOADING ALL LIBRARIES AND SETTING OPTIONS ####################
rm(list=ls())
library(dplyr)
library(stargazer)
options(scipen = 999)
# library(starpolishr) # library used to post-edit stargazer tables, but only works for LaTeX output
# install.packages("devtools")
# devtools::install_github("ChandlerLutz/starpolishr")
#################### LOADING ALL DATA AND PROCESSING ALL VARIABLES ####################
#################### Hyperlocal data (Tables 1 & 2)  ####################
##### Load Data
hyperlocal_data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv",
row.names=NULL)
##### Clean Variables
# median hh income (in $10 000s)
hyperlocal_data$med_hh_inc = gsub(",", "", hyperlocal_data$med_hh_inc)
hyperlocal_data$med_hh_inc = as.numeric(hyperlocal_data$med_hh_inc)
hyperlocal_data$med_hh_inc = hyperlocal_data$med_hh_inc / 10000 # median hh income in ten thousands
# poverty rate (%)
hyperlocal_data$poverty_rate = gsub("%", "", hyperlocal_data$poverty_rate)
hyperlocal_data$poverty_rate = as.numeric(hyperlocal_data$poverty_rate)
# unemployment rate (%)
hyperlocal_data$unemployment_rate = gsub("%", "", hyperlocal_data$unemployment_rate)
hyperlocal_data$unemployment_rate = as.numeric(hyperlocal_data$unemployment_rate)
# total population (in 10 000)
hyperlocal_data$total_population = gsub(",", "", hyperlocal_data$total_population)
hyperlocal_data$total_population = as.numeric(hyperlocal_data$total_population)
hyperlocal_data$total_population = hyperlocal_data$total_population / 10000 # total populatin in 10 000s
# for all included percentages: Hispanic, White, Black, Asian
hyperlocal_data$pct_hispanic_or_latino = gsub("%", "", hyperlocal_data$pct_hispanic_or_latino)
hyperlocal_data$pct_hispanic_or_latino = as.numeric(hyperlocal_data$pct_hispanic_or_latino)
hyperlocal_data$pct_white = gsub("%", "", hyperlocal_data$pct_white)
hyperlocal_data$pct_white = as.numeric(hyperlocal_data$pct_white)
hyperlocal_data$pct_black = gsub("%", "", hyperlocal_data$pct_black)
hyperlocal_data$pct_black = as.numeric(hyperlocal_data$pct_black)
hyperlocal_data$pct_asian = gsub("%", "", hyperlocal_data$pct_asian)
hyperlocal_data$pct_asian = as.numeric(hyperlocal_data$pct_asian)
hyperlocal_data$pct_american_indian = gsub("%", "", hyperlocal_data$pct_american_indian)
hyperlocal_data$pct_american_indian = as.numeric(hyperlocal_data$pct_american_indian)
hyperlocal_data$pct_hawaiian_pacific = gsub("%", "", hyperlocal_data$pct_hawaiian_pacific)
hyperlocal_data$pct_hawaiian_pacific = as.numeric(hyperlocal_data$pct_hawaiian_pacific)
hyperlocal_data$pct_other = gsub("%", "", hyperlocal_data$pct_other)
hyperlocal_data$pct_other = as.numeric(hyperlocal_data$pct_other)
hyperlocal_data$pct_two_or_more_races = gsub("%", "", hyperlocal_data$pct_two_or_more_races)
hyperlocal_data$pct_two_or_more_races = as.numeric(hyperlocal_data$pct_two_or_more_races)
##### Creating Log Outcome Variables - natural logarithm
# hyperlocal_data$NYC311_Calls_200m = log(hyperlocal_data$NYC311_Calls_200m)
# hyperlocal_data$NYC311_Calls_500m = log(hyperlocal_data$NYC311_Calls_500m)
# hyperlocal_data$NYC311_Calls_1000m = log(hyperlocal_data$NYC311_Calls_1000m)
# hyperlocal_data$Radius_200m = log(hyperlocal_data$Radius_200m)
# hyperlocal_data$Radius_500m = log(hyperlocal_data$Radius_500m)
# hyperlocal_data$Radius_1000m = log(hyperlocal_data$Radius_1000m)
# Checking for NA values
sum(is.na(hyperlocal_data$NYC311_Calls_200m))
sum(is.na(hyperlocal_data$NYC311_Calls_500m))
sum(is.na(hyperlocal_data$NYC311_Calls_1000m))
sum(is.na(hyperlocal_data$Radius_200m))
sum(is.na(hyperlocal_data$Radius_500m))
sum(is.na(hyperlocal_data$Radius_1000m))
# Removing rows with missing temeprature data
sum(is.na(hyperlocal_data$Daily_Temperature_.C.))
hyperlocal_data = hyperlocal_data[! is.na(hyperlocal_data$Daily_Temperature_.C.), ]
hist(hyperlocal_data$NYC311_Calls_200m, breaks = seq(min(hyperlocal_data$NYC311_Calls_200m), max(hyperlocal_data$NYC311_Calls_200m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
summary(hyperlocal_data$NYC311_Calls_200m)
summary(hyperlocal_data$NYC311_Calls_500m)
summary(hyperlocal_data$NYC311_Calls_1000m)
summary(hyperlocal_data$NYC311_Calls_200m)
summary(hyperlocal_data$NYC311_Calls_500m)
summary(hyperlocal_data$NYC311_Calls_1000m)
View(hyperlocal_data)
num_days_nocalls = hyperlocal_data %>%
group_by(Date) %>%
summarise(total_value = sum(NYC311_Calls_200m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Date) %>%
summarise(total_value = sum(NYC311_Calls_500m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Date) %>%
summarise(total_value = sum(Radius_200m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
sum(unique(hyperlocal_data$Sensor.ID))
nrow(unique(hyperlocal_data$Sensor.ID))
unique(hyperlocal_data$Sensor.ID)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(NYC311_Calls_200m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(NYC311_Calls_200m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(NYC311_Calls_500m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(NYC311_Calls_1000m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(Radius_200m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(Radius_500m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
num_days_nocalls = hyperlocal_data %>%
group_by(Sensor.ID, Date) %>%
summarise(total_value = sum(Radius_1000m)) %>%
filter(total_value == 0) %>%
nrow()
print(num_days_nocalls)
hist(hyperlocal_data$Radius_1000m, breaks = seq(min(hyperlocal_data$Radius_1000m), max(hyperlocal_data$Radius_1000m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
### PLOTTING HISTOGRAM OF ALL COUNT OUTCOME VARIABLES
# 200M RADIUS
hist(hyperlocal_data$Radius_1000m, breaks = seq(min(hyperlocal_data$Radius_200m), max(hyperlocal_data$Radius_1000m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
### PLOTTING HISTOGRAM OF ALL COUNT OUTCOME VARIABLES
# 200M RADIUS
hist(hyperlocal_data$Radius_200m, breaks = seq(min(hyperlocal_data$Radius_200m), max(hyperlocal_data$Radius_200m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
# 500M RADIUS
hist(hyperlocal_data$Radius_500m, breaks = seq(min(hyperlocal_data$Radius_500m), max(hyperlocal_data$Radius_500m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
# 1000M RADIUS
hist(hyperlocal_data$Radius_1000m, breaks = seq(min(hyperlocal_data$Radius_1000m), max(hyperlocal_data$Radius_1000m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
# 200M NEAREST
hist(hyperlocal_data$NYC311_Calls_200m, breaks = seq(min(hyperlocal_data$NYC311_Calls_200m), max(hyperlocal_data$NYC311_Calls_200m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
# 200M NEAREST
hist(hyperlocal_data$NYC311_Calls_500m, breaks = seq(min(hyperlocal_data$NYC311_Calls_500m), max(hyperlocal_data$NYC311_Calls_500m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
# 200M NEAREST
hist(hyperlocal_data$NYC311_Calls_1000m, breaks = seq(min(hyperlocal_data$NYC311_Calls_1000m), max(hyperlocal_data$NYC311_Calls_1000m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
#################### ALL REGRESSION RESULTS CODE: NOTES ####################
# since my outcome variables all have a high proportion of zeros,
# and since my outcome variables are all count variables that are over-dispersed (variance higher than mean),
# I will be using zero-inflated negative binomial regressions.
# I will be running several regressions:
# 1. Hyperlocal Monitor Radius specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv"
# 2. 1-1 specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv"
# 3. NTA specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nta_ALL_CONTROLS.csv"
# 4. AQS Monitor radius specification
# "C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Data/AQS related/AQS_nyc311_ALL_CONTROLS.csv"
# Each of these regressions take up a single table,
# each table has 2/3 panels for each distance range (200m, 500m, 1000m)
# Tables for 1. and 2. have the following 4 regressions and columns:
### baseline controls: just control for federal holidays
### baseline: add socioeconomic & race controls
### baseline: add weather controls
### baseline: add SBA fixed effects
# Table for 3. and 4. has the following 3 regressions and columns:
### baseline controls: just control for federal holidays
### baseline: add socioeconomic & race controls
### baseline: add weather controls
# can't add SBA fixed effects as the independent variable (temperature) would vary at the
# same level as the SBA fixed effect.
# for AQS regressions, there are only 2 monitors and 2 SBAs, and dependent variable
# would vary at the same level as SBA fixed effects, induce multicollinearity
#################### LOADING ALL LIBRARIES AND SETTING OPTIONS ####################
rm(list=ls())
library(dplyr)
library(stargazer)
options(scipen = 999)
# library(starpolishr) # library used to post-edit stargazer tables, but only works for LaTeX output
# install.packages("devtools")
# devtools::install_github("ChandlerLutz/starpolishr")
#################### LOADING ALL DATA AND PROCESSING ALL VARIABLES ####################
#################### Hyperlocal data (Tables 1 & 2)  ####################
##### Load Data
hyperlocal_data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nearest_and_radius_ALL_CONTROLS.csv",
row.names=NULL)
##### Clean Variables
# median hh income (in $10 000s)
hyperlocal_data$med_hh_inc = gsub(",", "", hyperlocal_data$med_hh_inc)
hyperlocal_data$med_hh_inc = as.numeric(hyperlocal_data$med_hh_inc)
hyperlocal_data$med_hh_inc = hyperlocal_data$med_hh_inc / 10000 # median hh income in ten thousands
# poverty rate (%)
hyperlocal_data$poverty_rate = gsub("%", "", hyperlocal_data$poverty_rate)
hyperlocal_data$poverty_rate = as.numeric(hyperlocal_data$poverty_rate)
# unemployment rate (%)
hyperlocal_data$unemployment_rate = gsub("%", "", hyperlocal_data$unemployment_rate)
hyperlocal_data$unemployment_rate = as.numeric(hyperlocal_data$unemployment_rate)
# total population (in 10 000)
hyperlocal_data$total_population = gsub(",", "", hyperlocal_data$total_population)
hyperlocal_data$total_population = as.numeric(hyperlocal_data$total_population)
hyperlocal_data$total_population = hyperlocal_data$total_population / 10000 # total populatin in 10 000s
# for all included percentages: Hispanic, White, Black, Asian
hyperlocal_data$pct_hispanic_or_latino = gsub("%", "", hyperlocal_data$pct_hispanic_or_latino)
hyperlocal_data$pct_hispanic_or_latino = as.numeric(hyperlocal_data$pct_hispanic_or_latino)
hyperlocal_data$pct_white = gsub("%", "", hyperlocal_data$pct_white)
hyperlocal_data$pct_white = as.numeric(hyperlocal_data$pct_white)
hyperlocal_data$pct_black = gsub("%", "", hyperlocal_data$pct_black)
hyperlocal_data$pct_black = as.numeric(hyperlocal_data$pct_black)
hyperlocal_data$pct_asian = gsub("%", "", hyperlocal_data$pct_asian)
hyperlocal_data$pct_asian = as.numeric(hyperlocal_data$pct_asian)
hyperlocal_data$pct_american_indian = gsub("%", "", hyperlocal_data$pct_american_indian)
hyperlocal_data$pct_american_indian = as.numeric(hyperlocal_data$pct_american_indian)
hyperlocal_data$pct_hawaiian_pacific = gsub("%", "", hyperlocal_data$pct_hawaiian_pacific)
hyperlocal_data$pct_hawaiian_pacific = as.numeric(hyperlocal_data$pct_hawaiian_pacific)
hyperlocal_data$pct_other = gsub("%", "", hyperlocal_data$pct_other)
hyperlocal_data$pct_other = as.numeric(hyperlocal_data$pct_other)
hyperlocal_data$pct_two_or_more_races = gsub("%", "", hyperlocal_data$pct_two_or_more_races)
hyperlocal_data$pct_two_or_more_races = as.numeric(hyperlocal_data$pct_two_or_more_races)
##### Creating Log Outcome Variables - natural logarithm
# hyperlocal_data$NYC311_Calls_200m = log(hyperlocal_data$NYC311_Calls_200m)
# hyperlocal_data$NYC311_Calls_500m = log(hyperlocal_data$NYC311_Calls_500m)
# hyperlocal_data$NYC311_Calls_1000m = log(hyperlocal_data$NYC311_Calls_1000m)
# hyperlocal_data$Radius_200m = log(hyperlocal_data$Radius_200m)
# hyperlocal_data$Radius_500m = log(hyperlocal_data$Radius_500m)
# hyperlocal_data$Radius_1000m = log(hyperlocal_data$Radius_1000m)
# Checking for NA values
sum(is.na(hyperlocal_data$NYC311_Calls_200m))
sum(is.na(hyperlocal_data$NYC311_Calls_500m))
sum(is.na(hyperlocal_data$NYC311_Calls_1000m))
sum(is.na(hyperlocal_data$Radius_200m))
sum(is.na(hyperlocal_data$Radius_500m))
sum(is.na(hyperlocal_data$Radius_1000m))
# Removing rows with missing temeprature data
sum(is.na(hyperlocal_data$Daily_Temperature_.C.))
hyperlocal_data = hyperlocal_data[! is.na(hyperlocal_data$Daily_Temperature_.C.), ]
hist(hyperlocal_data$NYC311_Calls_200m, breaks = seq(min(hyperlocal_data$NYC311_Calls_200m), max(hyperlocal_data$NYC311_Calls_200m)+1, by = 1), col = "skyblue", main = "Histogram", xlab = "Value", ylab = "Frequency")
#################### NTA data (Table 3)  ####################
NTA_data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Adding 3 Geographical Specifications/nta_ALL_CONTROLS.csv",
row.names=NULL)
# median hh income (in $10 000s)
NTA_data$med_hh_inc = gsub(",", "", NTA_data$med_hh_inc)
NTA_data$med_hh_inc = as.numeric(NTA_data$med_hh_inc)
NTA_data$med_hh_inc = NTA_data$med_hh_inc / 10000 # median hh income in ten thousands
# poverty rate (%)
NTA_data$poverty_rate = gsub("%", "", NTA_data$poverty_rate)
NTA_data$poverty_rate = as.numeric(NTA_data$poverty_rate)
# unemployment rate (%)
NTA_data$unemployment_rate = gsub("%", "", NTA_data$unemployment_rate)
NTA_data$unemployment_rate = as.numeric(NTA_data$unemployment_rate)
# total population (in 10 000)
NTA_data$total_population = gsub(",", "", NTA_data$total_population)
NTA_data$total_population = as.numeric(NTA_data$total_population)
NTA_data$total_population = NTA_data$total_population / 10000 # total populatin in 10 000s
# for all included percentages: Hispanic, White, Black, Asian
NTA_data$pct_hispanic_or_latino = gsub("%", "", NTA_data$pct_hispanic_or_latino)
NTA_data$pct_hispanic_or_latino = as.numeric(NTA_data$pct_hispanic_or_latino)
NTA_data$pct_white = gsub("%", "", NTA_data$pct_white)
NTA_data$pct_white = as.numeric(NTA_data$pct_white)
NTA_data$pct_black = gsub("%", "", NTA_data$pct_black)
NTA_data$pct_black = as.numeric(NTA_data$pct_black)
NTA_data$pct_asian = gsub("%", "", NTA_data$pct_asian)
NTA_data$pct_asian = as.numeric(NTA_data$pct_asian)
NTA_data$pct_american_indian = gsub("%", "", NTA_data$pct_american_indian)
NTA_data$pct_american_indian = as.numeric(NTA_data$pct_american_indian)
NTA_data$pct_hawaiian_pacific = gsub("%", "", NTA_data$pct_hawaiian_pacific)
NTA_data$pct_hawaiian_pacific = as.numeric(NTA_data$pct_hawaiian_pacific)
NTA_data$pct_other = gsub("%", "", NTA_data$pct_other)
NTA_data$pct_other = as.numeric(NTA_data$pct_other)
NTA_data$pct_two_or_more_races = gsub("%", "", NTA_data$pct_two_or_more_races)
NTA_data$pct_two_or_more_races = as.numeric(NTA_data$pct_two_or_more_races)
##### Creating Log Outcome Variables - natural logarithm
# NTA_data$nyc311_calls = log(NTA_data$nyc311_calls)
# Checking for NA values
sum(is.na(NTA_data$nyc311_calls))
sum(is.na(NTA_data$daily_ave_temp_C))
NTA_data = NTA_data[! is.na(NTA_data$daily_ave_temp_C), ]
#################### AQS data (Table 4)  ####################
AQS_data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5999 Masters Thesis/Thesis - Temperature & Social Activity/Data/AQS related/AQS_nyc311_ALL_CONTROLS.csv",
row.names = NULL)
AQS_data$Daily_Ave_Temp_C = as.numeric(AQS_data$Daily_Ave_Temp_C)
# median hh income (in $10 000s)
AQS_data$med_hh_inc = gsub(",", "", AQS_data$med_hh_inc)
AQS_data$med_hh_inc = as.numeric(AQS_data$med_hh_inc)
AQS_data$med_hh_inc = AQS_data$med_hh_inc / 10000 # median hh income in ten thousands
# poverty rate (%)
AQS_data$poverty_rate = gsub("%", "", AQS_data$poverty_rate)
AQS_data$poverty_rate = as.numeric(AQS_data$poverty_rate)
# unemployment rate (%)
AQS_data$unemployment_rate = gsub("%", "", AQS_data$unemployment_rate)
AQS_data$unemployment_rate = as.numeric(AQS_data$unemployment_rate)
# total population (in 10 000)
AQS_data$total_population = gsub(",", "", AQS_data$total_population)
AQS_data$total_population = as.numeric(AQS_data$total_population)
AQS_data$total_population = AQS_data$total_population / 10000 # total populatin in 10 000s
# for all included percentages: Hispanic, White, Black, Asian
AQS_data$pct_hispanic_or_latino = gsub("%", "", AQS_data$pct_hispanic_or_latino)
AQS_data$pct_hispanic_or_latino = as.numeric(AQS_data$pct_hispanic_or_latino)
AQS_data$pct_white = gsub("%", "", AQS_data$pct_white)
AQS_data$pct_white = as.numeric(AQS_data$pct_white)
AQS_data$pct_black = gsub("%", "", AQS_data$pct_black)
AQS_data$pct_black = as.numeric(AQS_data$pct_black)
AQS_data$pct_asian = gsub("%", "", AQS_data$pct_asian)
AQS_data$pct_asian = as.numeric(AQS_data$pct_asian)
AQS_data$pct_american_indian = gsub("%", "", AQS_data$pct_american_indian)
AQS_data$pct_american_indian = as.numeric(AQS_data$pct_american_indian)
AQS_data$pct_hawaiian_pacific = gsub("%", "", AQS_data$pct_hawaiian_pacific)
AQS_data$pct_hawaiian_pacific = as.numeric(AQS_data$pct_hawaiian_pacific)
AQS_data$pct_other = gsub("%", "", AQS_data$pct_other)
AQS_data$pct_other = as.numeric(AQS_data$pct_other)
AQS_data$pct_two_or_more_races = gsub("%", "", AQS_data$pct_two_or_more_races)
AQS_data$pct_two_or_more_races = as.numeric(AQS_data$pct_two_or_more_races)
##### Creating Log Outcome Variables - natural logarithm
# AQS_data$Radius_200m = log(AQS_data$Radius_200m)
# AQS_data$Radius_500m = log(AQS_data$Radius_500m)
# AQS_data$Radius_1000m = log(AQS_data$Radius_1000m)
# Checking for NA values
sum(is.na(AQS_data$Daily_Ave_Temp_C))
sum(is.na(AQS_data$Radius_200m))
sum(is.na(AQS_data$Radius_500m))
sum(is.na(AQS_data$Radius_1000m))
library(shiny); runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/Group GitHub Repository/GroupO_NYC311Complaints/jjyc_nyc ver3.R')
runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/Group GitHub Repository/GroupO_NYC311Complaints/jjyc_nyc ver3.R')
runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/Group GitHub Repository/GroupO_NYC311Complaints/jjyc_nyc ver3.R')
runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/Group GitHub Repository/GroupO_NYC311Complaints/jjyc_nyc ver3.R')
install.packages("rsconnect")
rsconnect::setAccountInfo(name='rayyy',
token='C46E85A9EE48BB058963274335F056A2',
secret='<SECRET>')
rsconnect::setAccountInfo(name='rayyy',
token='C46E85A9EE48BB058963274335F056A2',
secret='IqnvXA58CYAwDbzWR6F1uSYUSMBHo93IZr6gCVsJ')
rsconnect::deployApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/New Group GitHub Repository/Group_O_NYC311/nc_nyc.R')
library(shiny); runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/New Group GitHub Repository/Group_O_NYC311/nc_nyc.R')
rsconnect::deployApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/New Group GitHub Repository/Group_O_NYC311')
runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/New Group GitHub Repository/Group_O_NYC311/nc_nyc.R')
install.packages("quarto")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
csv_file_path = file.choose()
data=read.csv(csv_file_path)
colnames(data)
data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5018 Advanced Analytics/Adv Analytics Lab 3/temperature_nyc311.csv",
row.names = NULL)
data_subset = data[seq(1, nrow(data), by = 4), ]
library(dplyr)
library(estimatr)
library(miceadds)
options(scipen = 999)
test2 = lm.cluster(Radius_1000m~Daily_Temperature_.C. + med_hh_inc + poverty_rate + unemployment_rate + Borough,
cluster = Borough,
data=data_subset)
test2 = lm.cluster(Radius_1000m~Daily_Temperature_.C. + med_hh_inc + poverty_rate + unemployment_rate + Borough,
cluster = 'Borough',
data=data_subset)
install.packages("lmtest")
library(sandwich)
lbrary(lmtest)
library(lmtest)
test2 = lm.cluster(Radius_1000m~Daily_Temperature_.C. + med_hh_inc + poverty_rate + unemployment_rate + Borough,
cluster = 'Borough',
data=data_subset)
summary(test2)
library(lme4)
empty_reg = lmer(Radius_1000m~(1 | Borough),
data=data_subset,
REML=FALSE)
install.packages("Matrix")
install.packages("Matrix")
library(Matrix)
empty_reg = lmer(Radius_1000m~(1 | Borough),
data=data_subset,
REML=FALSE)
empty_reg = lmer(Radius_1000m~(1 | Borough),
data=data_subset)
empty_reg = lmer(Radius_1000m ~ (1 | Borough),
data=data_subset,
REML = FALSE)
data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5018 Advanced Analytics/Adv Analytics Lab 3/temperature_nyc311.csv",
row.names = NULL)
data_subset = data[seq(1, nrow(data), by = 4), ]
library(dplyr)
library(estimatr)
# library(miceadds)
options(scipen = 999)
test2 = lm.cluster(Radius_1000m~Daily_Temperature_.C. + med_hh_inc + poverty_rate + unemployment_rate + Borough,
cluster = 'Borough',
data=data_subset)
# 3. Then run an empty (random intercept) model.  Interpret it.
library(lme4)
install.packages("lme4", type = "source")
install.packages("lme4", type = "source")
# 3. Then run an empty (random intercept) model.  Interpret it.
library(lme4)
install.packages("Matrix")
install.packages("Matrix")
rm(list=ls())
data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5018 Advanced Analytics/Adv Analytics Lab 3/temperature_nyc311.csv",
row.names = NULL)
library(dplyr)
library(plm)
library(plyr)
data = read.csv("C:/Users/limre/Desktop/QMSS/Spring Semester/5018 Advanced Analytics/Adv Analytics Lab 3/temperature_nyc311.csv",
row.names = NULL)
library(plyr)
library(plm)
colnames(data)
ols = plm(Radius_1000m ~ Daily_Temperature_.C. + med_hh_inc + as.factor(year),
index = c("Sensor.ID", "year"),
data=data,
model="pooling")
ols = plm(Radius_1000m ~ Daily_Temperature_.C. + med_hh_inc + as.factor(year),
index = c("Sensor.ID", "Date"),
data=data,
model="pooling")
summary(ols)
fe = plm(Radius_1000m ~ Daily_Temperature_.C. + med_hh_inc + as.factor(year),
index = c("Sensor.ID", "Date"),
data=data,
model="within")
summary(fe)
# (c). Run a random effects model, too.  Run a Hausman test to compare the fixed and random effects. What does the Hausman test lead you to conclude?
re = plm(Radius_1000m ~ Daily_Temperature_.C. + med_hh_inc + as.factor(year),
index = c("Sensor.ID", "Date"),
data=data,
model="random")
summary(re)
phtest(fe, re)
fd = plm(Radius_1000m ~ Daily_Temperature_.C. + med_hh_inc + as.factor(year),
index = c("Sensor.ID", "Date"),
data=data,
model="fd")
summary(fd)
summary(re)
summary(re)
shiny::runApp('C:/Users/limre/Desktop/QMSS/Spring Semester/5063 Data Visualization/Data Viz Assignments/Final Group Project/New Group GitHub Repository/Group_O_NYC311')
